"""
Framework d'agent de RAG (Retrieval Augmented Generation) avec raffinement adaptatif de requ√™tes
bas√© sur les premiers r√©sultats et pr√©sentation d√©taill√©e des informations.
"""

import os
import time
import json
import asyncio
import random
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any, Set
from functools import partial

from pydantic import BaseModel, Field
from pydantic_ai import Agent, RunContext, ModelRetry
from pydantic_ai.models.mistral import MistralModel
from asyncpg import Pool, create_pool
import streamlit as st
import nest_asyncio

from utils import vectorize_query
from clients import client

# Active le support des boucles asyncio imbriqu√©es
nest_asyncio.apply()

#
# Mod√®les de donn√©es
#

class RefinedQuery(BaseModel):
    """Requ√™te raffin√©e g√©n√©r√©e √† partir des premiers r√©sultats"""
    text: str
    reason: str = Field(description="Justification de cette requ√™te suppl√©mentaire")
    expected_information: str = Field(description="Informations attendues de cette requ√™te")

class QueryRefinementResult(BaseModel):
    """R√©sultat de l'analyse des premiers documents et du raffinement de requ√™tes"""
    original_query: str
    refined_queries: List[RefinedQuery]
    analysis_summary: str = Field(description="R√©sum√© de l'analyse des documents initiaux")
    information_gaps: List[str] = Field(description="Lacunes d'information identifi√©es")

class Document(BaseModel):
    """Document r√©cup√©r√© de la base de donn√©es"""
    id: str
    content: str
    relevance_score: Optional[float] = None
    from_query: Optional[str] = None  # Trace la requ√™te source

class SearchResult(BaseModel):
    """R√©sultat de la recherche documentaire"""
    documents: List[Document]
    is_sufficient: bool = Field(description="Indique si les r√©sultats sont suffisants")
    feedback: Optional[str] = Field(None, description="Raison si insuffisant")

class DetailedSection(BaseModel):
    """Section d√©taill√©e de la r√©ponse finale"""
    title: str = Field(description="Titre de la section")
    content: str = Field(description="Contenu d√©taill√© de la section")
    sources: List[str] = Field(description="Sources utilis√©es dans cette section")

class EnhancedAnswer(BaseModel):
    """R√©ponse finale enrichie avec sections d√©taill√©es et mise en forme soign√©e"""
    summary: str = Field(description="R√©sum√© concis de la r√©ponse")
    sections: List[DetailedSection] = Field(description="Sections d√©taill√©es de la r√©ponse")
    key_insights: List[str] = Field(description="Points cl√©s √† retenir")
    sources: List[str] = Field(description="Toutes les sources utilis√©es")
    confidence: float = Field(ge=0.0, le=1.0, description="Niveau de confiance dans la r√©ponse")
    limitations: Optional[str] = Field(None, description="Limitations √©ventuelles de la r√©ponse")

@dataclass
class RetrievalDeps:
    """D√©pendances pour l'agent de r√©cup√©ration"""
    conn: Pool
    max_attempts: int = 2
    cache: Dict[str, Any] = None

@dataclass
class ProcessingState:
    """√âtat global du traitement d'une requ√™te"""
    original_query: str
    refined_queries: List[str] = field(default_factory=list)
    initial_documents: List[Document] = field(default_factory=list)
    all_documents: List[Document] = field(default_factory=list)
    processed_queries: Set[str] = field(default_factory=set)
    information_gaps: List[str] = field(default_factory=list)

# Cache pour les embeddings
embedding_cache = {}

#
# Configuration des mod√®les LLM
#

API_KEY = st.secrets['MISTRAL_API_KEY']
refinement_model = MistralModel(model_name='mistral-small-latest', api_key=API_KEY)
retrieval_model = MistralModel(model_name='mistral-small-latest', api_key=API_KEY)
presentation_model = MistralModel(model_name='mistral-large-latest', api_key=API_KEY)

#
# Fonctions utilitaires
#

async def get_embedding(query: str, max_retries=3) -> List[float]:
    """G√©n√®re un embedding vectoriel avec gestion du cache et des erreurs"""
    cache_key = query.lower().strip()

    if cache_key in embedding_cache:
        return embedding_cache[cache_key]

    retry_count = 0
    base_delay = 1

    while retry_count < max_retries:
        try:
            loop = asyncio.get_event_loop()
            embedding = await loop.run_in_executor(None, partial(vectorize_query, query), client)
            embedding_cache[cache_key] = embedding
            return embedding
        except Exception as e:
            if "rate limit" in str(e).lower() and retry_count < max_retries - 1:
                retry_count += 1
                print(f"Rate limit: attente {base_delay}s ({retry_count}/{max_retries})...")
                await asyncio.sleep(base_delay)
                base_delay *= 2
            else:
                print(f"Erreur d'embedding: {e}")
                return [random.uniform(-1, 1) for _ in range(1024)]


    return [random.uniform(-1, 1) for _ in range(1024)]

#
# Agent de r√©cup√©ration
#

retrieval_agent = Agent(
    model=retrieval_model,
    deps_type=RetrievalDeps,
    result_type=SearchResult,
    system_prompt="""Tu es un agent de r√©cup√©ration d'informations pertinentes.
√âvalue si les r√©sultats sont suffisants pour r√©pondre √† la question.
Sois critique et rigoureux.""",
    retries=3
)

@retrieval_agent.tool
async def search_docs(ctx: RunContext[RetrievalDeps], query: str, limit: int = 10) -> List[Document]:
    """Recherche des documents pertinents via embedding vectoriel"""
    cache_key = f"{query}:{limit}"

    if ctx.deps.cache and cache_key in ctx.deps.cache:
        return ctx.deps.cache[cache_key]

    start_time = time.time()
    try:
        query_embedding = await get_embedding(query)
        pg_vector = json.dumps(query_embedding)

        try:
            rows = await asyncio.wait_for(
                ctx.deps.conn.fetch(
                    """
                    SELECT id, id_doc, content, metadata, similarity
                    FROM match_mvp_docs($1, $2, $3, $4::jsonb, NULL)
                    """,
                    pg_vector, limit, 0.5, '{}'
                ),
                timeout=10.0
            )
        except asyncio.TimeoutError:
            print("Timeout de la requ√™te DB")
            rows = []

        documents = [
            Document(
                id=str(row['id']),
                content=row['content'],
                relevance_score=row.get('similarity', 0.5),
                from_query=query
            )
            for row in rows
        ]

        if ctx.deps.cache is not None:
            ctx.deps.cache[cache_key] = documents

        execution_time = time.time() - start_time
        print(f"Recherche '{query}' ex√©cut√©e en {execution_time:.2f}s - {len(documents)} documents trouv√©s")

        return documents

    except Exception as e:
        print(f"Erreur de recherche: {e}")
        return []

@retrieval_agent.result_validator
async def validate_search_results(ctx: RunContext[RetrievalDeps], result: SearchResult) -> SearchResult:
    """Valide la qualit√© des r√©sultats de recherche"""
    attempt = getattr(ctx, 'attempt', 1)

    if not result.documents:
        result.is_sufficient = False
        result.feedback = "Aucun document trouv√©"

        if attempt >= ctx.deps.max_attempts:
            result.is_sufficient = True
            result.feedback += " apr√®s plusieurs tentatives"
            return result

        setattr(ctx, 'attempt', attempt + 1)
        raise ModelRetry(f"Tentative {attempt}/{ctx.deps.max_attempts}: Aucun document trouv√©.")

    if any(doc.relevance_score and doc.relevance_score > 0.8 for doc in result.documents):
        result.is_sufficient = True
        return result

    if not result.is_sufficient and attempt >= ctx.deps.max_attempts:
        result.is_sufficient = True
        result.feedback += f" (apr√®s {ctx.deps.max_attempts} tentatives)"
        return result

    if not result.is_sufficient:
        setattr(ctx, 'attempt', attempt + 1)
        feedback = result.feedback or "R√©sultats insuffisants"
        raise ModelRetry(f"Tentative {attempt}/{ctx.deps.max_attempts}: {feedback}")

    return result

#
# Agent de raffinement de requ√™tes
#

query_refinement_agent = Agent(
    model=refinement_model,
    result_type=QueryRefinementResult,
    system_prompt="""Tu es un expert en analyse documentaire et raffinement de requ√™tes.

Ton objectif est d'analyser les premiers documents r√©cup√©r√©s pour une requ√™te et d'identifier:
1. Les informations d√©j√† disponibles
2. Les lacunes importantes qui n√©cessitent des recherches suppl√©mentaires
3. Les requ√™tes pr√©cises qui permettraient de combler ces lacunes

Pour chaque requ√™te raffin√©e que tu proposes:
- Assure-toi qu'elle est sp√©cifique et cibl√©e
- Explique pourquoi cette information manque dans les documents actuels
- Pr√©cise quelles informations tu esp√®res obtenir avec cette requ√™te

Ton analyse doit √™tre approfondie pour maximiser la pertinence des requ√™tes suppl√©mentaires.""",
    retries=2
)

#
# Agent de pr√©sentation am√©lior√©
#

presentation_agent = Agent(
    model=presentation_model,
    result_type=EnhancedAnswer,
    system_prompt="""Tu es un agent de pr√©sentation sp√©cialis√© dans la structuration et le formatage d'informations d√©taill√©es. Tu recevras du texte mais √©galement des tableaux.

Ton objectif est de cr√©er une r√©ponse riche et compl√®te qui:
1. Pr√©sente les informations de mani√®re claire et bien structur√©e
2. Ne n√©glige aucun d√©tail pertinent trouv√© dans les documents
3. Organise le contenu en sections th√©matiques avec titres explicites
4. Fournit un r√©sum√© concis mais complet en introduction
5. Met en √©vidence les points cl√©s √† retenir
6. Cite rigoureusement toutes les sources utilis√©es
7. Indique les √©ventuelles limitations de la r√©ponse

Concernant les tableaux sp√©cifiquement:
- Tu dois les restructurer pour optimiser leur lisibilit√© et leur compr√©hension
- Assure-toi que les en-t√™tes sont clairs et pertinents
- Utilise le formatage appropri√© (alignement, espacement) pour am√©liorer la lisibilit√©
- Ajoute des notes explicatives si certaines donn√©es du tableau n√©cessitent des clarifications
- Int√®gre harmonieusement les tableaux dans la structure globale de ta r√©ponse
- N'h√©site pas √† transformer un tableau en liste √† puces ou en paragraphes si cela am√©liore la compr√©hension

Ta priorit√© est d'offrir une pr√©sentation soign√©e qui valorise la qualit√© des informations trouv√©es tout en maximisant la couverture des d√©tails pertinents. Le traitement optimal des tableaux est essentiel pour atteindre cet objectif.""",
    retries=2
)

def preprocess_documents(documents: List[Document], max_chars: int = 1500) -> List[Document]:
    """Tronque les documents pour limiter les tokens tout en pr√©servant plus de contenu"""
    return [
        Document(
            id=doc.id,
            content=doc.content[:max_chars] + ("..." if len(doc.content) > max_chars else ""),
            relevance_score=doc.relevance_score,
            from_query=doc.from_query
        )
        for doc in documents
    ]

def deduplicate_documents(documents: List[Document]) -> List[Document]:
    """√âlimine les documents en double bas√©s sur leur ID"""
    seen_ids = set()
    unique_docs = []

    for doc in documents:
        if doc.id not in seen_ids:
            seen_ids.add(doc.id)
            unique_docs.append(doc)

    return unique_docs

def categorize_documents(documents: List[Document]) -> Dict[str, List[Document]]:
    """Organise les documents par requ√™te source pour faciliter la structuration"""
    categories = {}

    for doc in documents:
        query = doc.from_query or "Requ√™te principale"
        if query not in categories:
            categories[query] = []
        categories[query].append(doc)

    return categories

async def init_db_pool() -> Pool:
    """Initialise la connexion √† la base de donn√©es"""
    return await create_pool(
        st.secrets['SUPABASE_PG_URL'],
        min_size=5,
        max_size=15,
        statement_cache_size=0,
        timeout=120.0,
        command_timeout=10.0
    )

def get_streamlit_deps(pool):
    """Cr√©e des d√©pendances sp√©cifiques pour l'environnement Streamlit avec plus de tentatives"""
    return RetrievalDeps(
        conn=pool,
        max_attempts=5,  # Plus de tentatives
        cache={}
    )

async def process_query(question: str, pool: Pool) -> EnhancedAnswer:
    """Traite une requ√™te utilisateur avec raffinement adaptatif et pr√©sentation enrichie"""
    state = ProcessingState(original_query=question)
    cache = {}
    deps = RetrievalDeps(conn=pool, cache=cache, max_attempts=2)

    try:
        # √âtape 1: Recherche initiale avec la requ√™te originale
        print(f"Recherche initiale: {question}")
        retrieval_start = time.time()

        try:
            initial_retrieval = await asyncio.wait_for(
                retrieval_agent.run(question, deps=deps),
                timeout=45.0
            )

            initial_documents = initial_retrieval.data.documents
            state.initial_documents = initial_documents
            state.all_documents = initial_documents.copy()
            state.processed_queries.add(question)

            retrieval_time = time.time() - retrieval_start
            print(f"Recherche initiale termin√©e en {retrieval_time:.2f}s - {len(initial_documents)} documents trouv√©s")

            if not initial_documents:
                print("Aucun document trouv√© lors de la recherche initiale")
                return EnhancedAnswer(
                    summary="Aucune information pertinente n'a √©t√© trouv√©e pour r√©pondre √† cette question.",
                    sections=[],
                    key_insights=["Aucune donn√©e disponible"],
                    sources=[],
                    confidence=0.0,
                    limitations="Absence de documents pertinents dans la base de connaissances."
                )

        except Exception as e:
            print(f"Erreur lors de la recherche initiale: {e}")
            return EnhancedAnswer(
                summary=f"Une erreur s'est produite lors de la recherche initiale: {str(e)}",
                sections=[],
                key_insights=["Erreur technique lors de la recherche"],
                sources=[],
                confidence=0.0,
                limitations="Erreur technique lors de la recherche initiale"
            )

        # √âtape 2: Raffinement des requ√™tes bas√© sur les premiers r√©sultats
        print("Raffinement des requ√™tes bas√© sur les premiers r√©sultats...")
        refinement_start = time.time()

        try:
            # Pr√©paration des documents initiaux pour le raffinement
            processed_initial_docs = preprocess_documents(initial_documents)

            refinement_prompt = {
                "original_query": question,
                "documents_initiaux": [doc.model_dump() for doc in processed_initial_docs],
                "nombre_documents": len(processed_initial_docs)
            }

            refinement_result = await asyncio.wait_for(
                query_refinement_agent.run(json.dumps(refinement_prompt)),
                timeout=60.0
            )

            # Extraction des requ√™tes raffin√©es
            state.refined_queries = [rq.text for rq in refinement_result.data.refined_queries]
            state.information_gaps = refinement_result.data.information_gaps

            refinement_time = time.time() - refinement_start
            print(f"Raffinement termin√© en {refinement_time:.2f}s")
            print(f"Requ√™tes raffin√©es g√©n√©r√©es: {len(state.refined_queries)}")

            for i, rq in enumerate(refinement_result.data.refined_queries, 1):
                print(f"  {i}. {rq.text}")
                print(f"     ‚Ü≥ Raison: {rq.reason}")
                print(f"     ‚Ü≥ Information attendue: {rq.expected_information}")

            print("\nR√©sum√© de l'analyse initiale:")
            print(refinement_result.data.analysis_summary)

            print("\nLacunes d'information identifi√©es:")
            for gap in refinement_result.data.information_gaps:
                print(f"- {gap}")

        except Exception as e:
            print(f"Erreur lors du raffinement des requ√™tes: {e}")
            # En cas d'√©chec du raffinement, on continue avec les documents initiaux
            state.refined_queries = []

        # √âtape 3: Ex√©cution des requ√™tes raffin√©es
        if state.refined_queries:
            print("\nEx√©cution des requ√™tes raffin√©es...")

            for query in state.refined_queries:
                if query in state.processed_queries:
                    continue

                try:
                    retrieval_result = await asyncio.wait_for(
                        retrieval_agent.run(query, deps=deps),
                        timeout=30.0
                    )

                    if retrieval_result.data.documents:
                        state.all_documents.extend(retrieval_result.data.documents)

                    state.processed_queries.add(query)

                except Exception as e:
                    print(f"Erreur lors de la recherche pour '{query}': {e}")

        # √âtape 4: D√©duplication et pr√©paration des documents
        unique_documents = deduplicate_documents(state.all_documents)
        print(f"\nDocuments r√©cup√©r√©s: {len(state.all_documents)} ‚Üí {len(unique_documents)} apr√®s d√©duplication")

        if not unique_documents:
            return EnhancedAnswer(
                summary="Malgr√© plusieurs tentatives, aucune information pertinente n'a √©t√© trouv√©e.",
                sections=[],
                key_insights=["Aucune donn√©e disponible malgr√© le raffinement des requ√™tes"],
                sources=[],
                confidence=0.0,
                limitations="Absence de documents pertinents dans la base de connaissances."
            )

        # √âtape 5: Pr√©sentation enrichie
        processed_docs = preprocess_documents(unique_documents, max_chars=1500)
        categorized_docs = categorize_documents(processed_docs)

        # Construction d'un prompt enrichi pour l'agent de pr√©sentation
        prompt = {
            "question_originale": question,
            "requetes_executees": list(state.processed_queries),
            "lacunes_identifiees": state.information_gaps,
            "documents_par_categorie": {
                category: [doc.model_dump() for doc in docs]
                for category, docs in categorized_docs.items()
            },
            "nombre_total_documents": len(processed_docs),
            "instructions_particulieres": "Privil√©gier la richesse des d√©tails tout en pr√©sentant l'information de mani√®re structur√©e et √©l√©gante."
        }

        presentation_start = time.time()
        final_result = await asyncio.wait_for(
            presentation_agent.run(json.dumps(prompt)),
            timeout=60.0
        )

        print(f"Pr√©sentation enrichie termin√©e en {time.time() - presentation_start:.2f}s")
        return final_result.data

    except asyncio.TimeoutError:
        print("Timeout global de l'op√©ration")
        return EnhancedAnswer(
            summary="Le traitement a pris trop de temps et n'a pas pu √™tre compl√©t√©.",
            sections=[],
            key_insights=["Op√©ration interrompue pour cause de d√©lai excessif"],
            sources=[],
            confidence=0.0,
            limitations="Timeout de l'op√©ration"
        )
    except Exception as e:
        print(f"Erreur lors du traitement: {e}")
        return EnhancedAnswer(
            summary="Une erreur s'est produite lors du traitement de la requ√™te.",
            sections=[],
            key_insights=["Erreur technique rencontr√©e"],
            sources=[],
            confidence=0.0,
            limitations=f"Erreur technique: {str(e)}"
        )

# async def main():
#     """Point d'entr√©e principal"""
#     pool = None
#     try:
#         print("Initialisation de la connexion √† la base de donn√©es...")
#         pool = await asyncio.wait_for(init_db_pool(), timeout=15.0)

#         question = ""
#         start_time = time.time()

#         result = await process_query(question, pool)

#         # Affichage format√© du r√©sultat
#         print("\n" + "="*80)
#         print(f"üìå R√âSUM√â: {result.summary}")
#         print("="*80)

#         # Affichage des sections d√©taill√©es
#         for i, section in enumerate(result.sections, 1):
#             print(f"\n## {i}. {section.title}")
#             print(f"{section.content}")
#             print(f"Sources: {', '.join(section.sources)}")

#         # Points cl√©s
#         print("\n" + "-"*80)
#         print("üîë POINTS CL√âS:")
#         for i, insight in enumerate(result.key_insights, 1):
#             print(f"  {i}. {insight}")

#         # M√©tadonn√©es
#         print("\n" + "-"*80)
#         print(f"üìö Sources utilis√©es: {', '.join(result.sources)}")
#         print(f"üéØ Niveau de confiance: {result.confidence * 100:.1f}%")

#         if result.limitations:
#             print(f"‚ö†Ô∏è Limitations: {result.limitations}")

#         print("-"*80)
#         print(f"‚è±Ô∏è Temps total: {time.time() - start_time:.2f}s")

#         return result

#     finally:
#         if pool:
#             try:
#                 await asyncio.wait_for(pool.close(), timeout=5.0)
#             except Exception as e:
#                 print(f"Probl√®me de fermeture du pool: {e}")


def execute_mode_profond(question: str) -> EnhancedAnswer:
    """
    Fonction synchrone qui permet d'appeler facilement le workflow d'agent
    depuis une application Streamlit.

    Args:
        question (str): La question de l'utilisateur

    Returns:
        EnhancedAnswer: La r√©ponse structur√©e
    """
    async def run_query():
        # Initialise la connexion √† la base de donn√©es
        pool = await init_db_pool()
        try:
            return await process_query(question, pool)
        finally:
            # Gestion plus robuste de la fermeture du pool
            try:
                # Utilisation de asyncio.shield pour √©viter que le timeout n'annule l'op√©ration de fermeture
                close_task = asyncio.shield(pool.close())
                await asyncio.wait_for(close_task, timeout=90.0)
            except asyncio.TimeoutError:
                print("Attention: Timeout lors de la fermeture du pool, mais l'op√©ration continue en arri√®re-plan")
            except Exception as e:
                print(f"Probl√®me de fermeture du pool: {e}")

    # Ex√©cute la fonction asynchrone et retourne le r√©sultat
    return asyncio.run(run_query())
