from typing import List, Dict, Any
import time
import nest_asyncio
import asyncio

from mistralai import Mistral, UserMessage
import psycopg2
import streamlit as st

from s3_client import upload_to_minio
from utils import retrieve_context, vectorize_query, format_chunks_with_bullets, system_prompt
from mode_profond import execute_mode_profond
from clients import client

# Active le support des boucles asyncio imbriqu√©es (important pour Streamlit)
nest_asyncio.apply()

db_connection_string = st.secrets['SUPABASE_PG_URL']
supabase_document_table = st.secrets['SUPABASE_TABLE']
conn = psycopg2.connect(db_connection_string)

st.title("Le Ragueauteur")

st.text("Le Ragueauteur vous permet de poser n'importe quelle question en lien avec le syst√®me de gestion des donn√©es des eaux souterraines.")

# Set default model
if "mistral_model" not in st.session_state:
    st.session_state["mistral_model"] = "mistral-large-latest"

#Add "mode" to the session state
if 'mode' not in st.session_state:
    st.session_state['mode'] = 'concis'

# Store chat messages
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

#Configuration bouton + √©tat suivant
option_names = ['concis', 'profond']

mode_button = st.button(f"Mode {st.session_state['mode']}")
next_mode = 'profond' if st.session_state['mode'] == 'concis' else 'concis'

# Logique quand le bouton est cliqu√©
if mode_button:
    st.session_state['mode'] = next_mode
    st.rerun()

# User input
if prompt := st.chat_input("Besoin de renseignement ?"):
    if st.session_state['mode'] == 'concis':

        st.session_state.messages.append({"role": "user", "content": prompt})

        input_vector = vectorize_query(prompt, client)

        context = retrieve_context(conn, supabase_document_table, input_vector, 10)
        context = format_chunks_with_bullets(context, prompt)
        with st.chat_message("user"):
            st.markdown(prompt)

        system_message = {"role": "system", "content": f"{system_prompt}\n\nContext: {context}"}
        messages = [system_message] + [{"role": m["role"], "content": m["content"]} for m in st.session_state.messages]

        # Call Mistral API
        with st.chat_message("assistant", avatar="images/logoRageau.jpg"):
            stream_response = client.chat.stream(
                model=st.session_state["mistral_model"],
                messages=messages,
            )
            full_response = ""
            placeholder = st.empty()  # Cr√©e un espace r√©serv√© pour la r√©ponse
            for chunk in stream_response:
                content = chunk.data.choices[0].delta.content
                if content:
                    full_response += content
                    placeholder.markdown(full_response + "‚ñå", unsafe_allow_html=True)  # Affiche la r√©ponse partielle
                    time.sleep(0.05)  # Petite pause pour rendre l'affichage plus naturel
            placeholder.markdown(full_response, unsafe_allow_html=True)  # Affiche la r√©ponse compl√®te

        st.session_state.messages.append({"role": "assistant", "content": full_response})

    # Mode profond (nouveau)
    elif st.session_state['mode'] == 'profond':
        start_time = time.time()  # Pour calculer le temps total

        # Ajouter le message de l'utilisateur √† l'historique et l'afficher
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant", avatar="images/logoRageau.jpg"):
            with st.spinner("Analyse approfondie en cours..."):
                # Appel au workflow d'agent
                resultat = execute_mode_profond(prompt)

                # Banni√®re de r√©sum√©
                st.markdown("---")
                st.markdown("## üìå R√âSUM√â")
                st.markdown(resultat.summary)
                st.markdown("---")

                # Sections d√©taill√©es
                for i, section in enumerate(resultat.sections, 1):
                    with st.expander(f"{i}. {section.title}"):
                        st.markdown(section.content)
                        st.caption(f"Sources: {', '.join(section.sources)}")

                # Points cl√©s
                if resultat.key_insights:
                    st.markdown("## üîë POINTS CL√âS")
                    for i, insight in enumerate(resultat.key_insights, 1):
                        st.markdown(f"**{i}.** {insight}")

                # M√©tadonn√©es
                st.markdown("---")
                col1, col2 = st.columns(2)
                with col1:
                    if resultat.sources:
                        st.markdown(f"**üìö Sources utilis√©es:** {len(resultat.sources)}")
                        with st.expander("Voir les sources"):
                            for src in resultat.sources:
                                st.markdown(f"- {src}")

                with col2:
                    st.markdown(f"**üéØ Niveau de confiance:** {resultat.confidence * 100:.1f}%")

                    # Afficher le temps total
                    elapsed_time = time.time() - start_time
                    st.markdown(f"**‚è±Ô∏è Temps total:** {elapsed_time:.2f}s")

                # Limitations si pr√©sentes
                if resultat.limitations:
                    st.warning(f"**‚ö†Ô∏è Limitations:** {resultat.limitations}")

            # Cr√©ation de la r√©ponse pour l'historique sans utiliser de f-string complexe
            full_response = "## üìå R√âSUM√â\n" + resultat.summary + "\n\n"

            # Ajouter les sections
            full_response += "## SECTIONS D√âTAILL√âES\n"
            for section in resultat.sections:
                full_response += f"### {section.title}\n{section.content}\n\n"

            # Ajouter les points cl√©s
            full_response += "## üîë POINTS CL√âS\n"
            for insight in resultat.key_insights:
                full_response += f"- {insight}\n"

            # Ajouter les m√©tadonn√©es
            full_response += f"\n## M√âTADONN√âES\n"
            full_response += f"- üìö Sources: {len(resultat.sources)}\n"
            full_response += f"- üéØ Confiance: {resultat.confidence * 100:.1f}%\n"

            # Ajouter les limitations si pr√©sentes
            if resultat.limitations:
                full_response += f"\n‚ö†Ô∏è **Limitations:** {resultat.limitations}"

        # Mise √† jour de l'historique
        st.session_state.messages.append({"role": "assistant", "content": full_response})


# Interface Streamlit pour drag and drop
uploaded_file = st.file_uploader("Glissez et d√©posez votre fichier", type=["pdf"])

    # Si un fichier est t√©l√©charg√©
if uploaded_file is not None:
    st.write(f"Fichier t√©l√©charg√© : {uploaded_file.name}")
    # Affiche un bouton pour valider l'upload
    if st.button("Valider l'upload sur MinIO"):
        # Appel de la fonction pour uploader le fichier sur MinIO
        file_name = upload_to_minio(uploaded_file)

